<!doctype html>

<html lang="en">
<head>
    <meta charset="utf-8">

    <title>math189r</title>
    <meta name="description" content="Harvey Mudd Mathematics of Big Data I">
    
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>

    <div class="header">
        <h2> <a href="index.html">math189r</a> </h2>
        <h3>
            <a href="info/">Info</a> |
            <a href="hw/">Homework</a> |
            <a href="materials/">Materials</a>
        </h3>
    </div>

    <div class="body">
        <h3>Mathematics of Big Data I</h3>
        <p>
            Professor Weiqing Gu</br>
            Harvey Mudd College</br>
            Fall 2016
        </p>

<table>
    <thead>
        <tr>
            <th>Supervised</br>Learning</th>
            <th>Monday</th>
            <th>Thursday</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><a href="">Sept. 5</a></td>
            <td>
                Introduction. Linear Regression. Normal
                Equations and Optimization Techniques.
                Robust Linear Regression. Ridge Regression.
                Big Data as a Regularizer. </br>

                <b>Read:</b> Murphy, 7.{1,...,5}
            </td>
            <td>
                <a href="materials/sec_linear.pdf">Linear Algebra Review.</a>
            </td>
        </tr>
        <tr>
            <td><a href="">Sept. 12</a></td>
            <td>
                Classification. Logistic Regression. Exponential
                Family and Generalized Linear Models. Logistic
                Regression as a GLM.</br>

                <b>Read:</b> Murphy, 8.{1,2,3,5} \ 8.{3.4,3.5}, 9.{1,2.2,2.4,3}
            </td>
            <td>
                <a href="materials/sec_probability.pdf">Probability Review.</a></br>
            </td>
        </tr>
        <tr>
            <td><a href="">Sept. 19</a></td>
            <td>
                Generalized Linear Models continued.
                Poisson Regression. Softmax Regression.</br>

                <b>Read:</b> Murphy {}</br>
                <b>Due:</b> <a href="homework/sept_19.pdf">Homework 1</a>
            </td>
            <td>
                <a href="materials/sec_convex.pdf">Convex Optimization</br>Overview I</a>
            </td>
        </tr>
        <tr>
            <td><a href="">Sept. 26</a></td>
            <td>
                Generative Learning Algorithms.
                Gaussian Discriminant Analysis. Naive Bayes.</br>

                <b>Read:</b> Murphy 4.{1,2,3,4} \ 4.{3.3}</br>
            </td>
            <td>
                <a href="materials/sec_convex.pdf">Convex Optimization</br>Overview II</a>
            </td>
        </tr>
        <tr>
            <td><a href="">Oct.  3</a></td>
            <td>
                Support Vector Machines. Kernels.</br>

                <b>Read:</b> Murphy {}</br>
                <b>Read:</b> <a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">MapReduce: Simplified Data Processing on Large Clusters</a> (DEFINITELY READ THIS!)</br>
                <b>Due:</b> <a href="homework/oct_3.pdf">Homework 2</a>
            </td>
            <td>
                <a href="materials/sec_mapreduce.pdf">MapReduce and</br>Distributed Computatio</br> and Learning</a>
            </td>
        </tr>
        <tr>
            <td><b> Unsupervised Learning </b></br><a href="">Oct.  10</a></td>
            <td>
                Introduction to Unsupervised Learning.
                Clustering. K-Means. Mixture of Gaussians.
                Expectation-Maximization (EM) Algorithm.</br>

                <b>Read:</b> <a href="http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf">Pegasos: Primal Estimated sub-GrAdient SOlver for SVM</a>
            </td>
            <td>
                <a href="materials/sec_spam.pdf">Applications: Spam Detection</br>and Sentiment Analysis</a>
            </td>
        </tr>
        <tr>
            <td><a href="">Oct.  17</a></td>
            <td>Fall Break</td>
            <td>Fall Break</td>
        </tr>
        <tr>
            <td><a href="">Oct.  24</a></td>
            <td>
                Principal Component Analysis (PCA).
                Kernel PCA. One Class Support Vector
                Machines. </br>

                <b>Read:</b> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.575&rep=rep1&type=pdf"> Support Vector Method for Novelty Detection</a></br>
                <b>Due:</b> Midterm. Final Project Progress Report.
            </td>
            <td>
                <a href="materials/">SVM Training: Distributed</br>and Locally</a>
            </td>
        </tr>
        <tr>
            <td><a href="">Oct.  31</a></td>
            <td>
                Learning Theory. VC Dimension. Bias/Variance
                Trade-off. Union and Chernoff/Hoeffding
                Bounds.

                <b>Read:</b> <a href="http://202.120.81.220:81/inter/uploads/readings/Detecting%20Adversarial%20Advertisements%20in%20the%20Wild.pdf">Detecting Adversarial Advertisements in the Wild</a></br>
                <b>Due:</b> <a href="homework/nov_7.pdf">Homework 3</a>
            </td>
            <td>
                <a href="materials/sec_hmm.pdf">Hidden Markov Models</a>
            </td>
        </tr>
        <tr>
            <td><b> Recommender Systems </b></br><a href="">Nov.  7</a></td>
            <td>
                Introduction to Recommender Systems. Collaborative
                Filtering. Non-Negative Matrix Factorization.</br>

                <b>Read:</b> <a href="http://sifter.org/~simon/journal/20061211.html">Netflix Update: Try This at Home</a></br>
            </td>
            <td>
                <a href="materials/sec_music.pdf">Application: Music Reccomendation</a>
            </td>
        </tr>
        <tr>
            <td><b>Bayesian Learning</b></br><a href="">Nov.  14</a></td>
            <td>
                Introduction to Bayesian Reasoning.
                Bayesian Linear Regression. Bayesian
                Logistic Regression.</br>

                <b>Read:</b> Murphy 5.{1,2,3.0,3.2} 7.6, 8.4</br>
                <b>Due:</b> <a href="homework/nov_14.pdf">Homework 4</a>
            </td>
            <td></td>
        </tr>
        <tr>
            <td><a href="">Nov.  21</a></td>
            <td>
                Nonparametric Models. Gaussian Processes. </br>
            
                <b>Read:</b> Murphy 15.{1,2}</br>
            </td>
            <td>Thanksgiving</td>
        </tr>
        <tr>
            <td><a href="">Nov.  28</a></td>
            <td>
                Monte-Carlo Methods. Importance Sampling.
                Markov-Chain Monte-Carlo.</br>

                <b>Read:</b> <a href="http://jmlr.org/proceedings/papers/v28/wilson13.pdf">Gaussian Process Kernels for Pattern Discovery and Extrapolation</a></br>
                <b>Due:</b> <a href="homework/nov_14.pdf">Homework 4</a>
            </td>
            </td>
            <td></td>
        </tr>
        <tr>
            <td><a href="">Dec.  5</a></td>
            <td>
                Gibbs Sampling. The Metropolis-Hastings
                Algorithm. Latent Dirichlet Allocation.</br>
            </td>
            <td></td>
        </tr>
        <tr>
            <td>Dec.  12</td>
            <td>Finals</td>
            <td>Finals</td>
        </tr>
    </tbody>
</table>
        
    </div>

</body>
</html>
