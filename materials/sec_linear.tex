\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{enumerate,mathtools}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{hyperref}

\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\ii}{\mathbf{i}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\renewcommand{\SS}{\mathbb{S}}
\renewcommand{\aa}{\mathbf{a}}
\newcommand{\T}{\top}
\newcommand{\rank}{\mathrm{rank}}

\newcommand{\m}[1]{\begin{bmatrix} #1 \end{bmatrix}}

%\renewcommand{\section}[1]{\Large\textbf{#1}}

\setlength{\parindent}{0pt}

\begin{document}

\begin{center}
  \Large\textbf{Linear Algebra Review}\\
  \large\textit{Conner DiPaolo}
\end{center}
\vspace*{1em}

\tableofcontents
\vspace{1em}

This is a conglomeration of Stanford's CS229 \textit{Linear Algebra Review and Reference}
\footnote{\url{http://cs229.stanford.edu/section/cs229-linalg.pdf}}, written by Zico Kolter
and updated by Chuong Do, and some extra material that will be helpful in the
course.

\section{Basics}

Linear Algebra allows us to interact with linear operators (or systems
of linear equations) in a more powerful manner than dealing with equations.
For example, consider the linear system
\begin{align*}
    x_1 + 2x_2 &= 5\\
    3x_1 + 4x_2 &= 1.
\end{align*}
This can be solved for $x_1$ and $x_2$ using substitution, but it is
convenient (for many reasons) to investigate this system more compactly,
as a matrix-vector product. Namely,
\[
    A\xx = \bb
\]
where
\[
    A = \m{1&2\\3&4} \quad\text{and}\quad \bb = \m{5\\1}.
\]
Here, $A$ is a \textit{matrix} and $\bb$ is a \textit{vector}. Matrices are
\textit{linear operators}. That is, they obey the following property, where
$A$ is a matrix in $\RR^{m\times n}$, $\xx$ and $\yy$ are vectors in $\RR^n$,
and $c$ and $d$ are scalars:
\[
    A(c\xx+d\yy) = cA\xx + dA\yy.
\]
Note that this implies that
\[
    A\0 = \0.
\]

\subsection{Notation}

\begin{enumerate}[-]
    \item We denote an $m\times n$ matrix of real numbers $A$ as $A\in\RR^{m\times n}$
        (``A is in R m by n'').
        Similarly, to declare a $m\times n$ matrix of complex numbers we say $B\in\CC^{m\times n}$.
    \item We denote a vector $\xx$ with $n$ real elements as $x\in\RR^n$. By convention, $\xx$
        is assumed to be a column vector (that is, equivalently, $\xx\in\RR^{n\times 1}$).
        If we want to represent a row vector, we use $\xx^\T$, where $\T$ is the transpose.
    \item The $i-$th element of a vector $\xx$ is denoted $x_i$.
    \item We denote the $i,j$-th element of a matrix $A$ as $a_{ij}$, $A_{ij}$, or
        $A_{i,j}$.
        \[
            A = \m{a_{11} & a_{12}&\dots&a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n}\\ \vdots&\vdots&\ddots&\vdots\\a_{m1}&a_{m2}&\dots&a_{mn}}
        \]
    \item We denote the $j-$th column of $A$ as $A_{:,j}$.
    \item We denote the $i-$th row of $A$ as $A_{i,:}$.
    \item $\1$ is the vector of all ones. $\0$ is the vector of
        all zeroes. Size is dependent on context.
\end{enumerate}

\section{Matrix Multiplication}

From the intro section we have already seen a matrix-vector product $A\xx$. We will
now define vector-vector products (the inner and outer product) and matrix-matrix
products (which encapsulate matrix-vector products).\\

The product of two matrices $A\in\RR^{m\times n}$ and $B\in\RR^{n\times p}$ is
the matrix
\[
    C = AB\in\RR^{m\times p}
\]
where
\[
    C_{ij} = \sum_{k=1}^n A_{ik}B_{kj}.
\]

\subsection{Vector-Vector Products}

Let $\xx,\yy\in\RR^n$. Then the \textit{inner product} (sometimes referred as the
dot product but we won't use that terminology)
\[
    \langle \xx,\yy \rangle = \xx^\T\yy = \sum_{i=1}^n \xx_i\yy_i = ||\xx||_2||\yy||_2\cos\theta
\]
where $\theta$ is the angle between the vectors.
Note that $\langle \cdot, \cdot \rangle: (V,V) \mapsto \RR$ (``$\langle \cdot, \cdot
\rangle$ maps two elements of the vector space $V$ to $\RR$'') here is the standard definition of
the inner product for the vector space $V=\RR^n$. As we will see later in the
class, other inner products can be defined between vectors in $\RR^n$.\\

The \textit{outer product} between $\xx\in\RR^m$ and $\yy\in\RR^n$ as
\[
    \xx\yy^\T \in \RR^{m\times n},
\]
a matrix.
\[
    (\xx\yy^\T)_{ij} = \xx_i\yy_j.
\]
This will be useful when we talk about Principal Component Analysis and
Covariance.

\subsection{Matrix-Vector Products}

The matrix-vector product between $A\in\RR^{m\times n}$ and $\xx\in\RR^n$ is
the vector $\yy = A\xx \in \RR^m$. We can see from the formula for matrix-matrix
multiplication that
\[
    A\xx = \m{|&|& &|\\a_1&a_2&\dots&a_n\\|&|& &|}\m{x_1\\x_2\\\dots\\x_n} = x_1\m{|\\a_1\\|} + x_2\m{|\\a_2\\|} + \dots + x_n\m{|\\x_n\\|},
\]
a linear combination of the columns of $A$!

\subsection{Matrix-Matrix Multiplication}

Armed with this knowledge, we can see matrix-matrix multiplication between
$A\in\RR^{m\times n}$ and $B\in\RR^{n\times p}$ as

\[
    AB = \m{|&|& &|\\a_1&a_2&\dots&a_n\\|&|& &|}\m{\horzbar&b_1^T&\horzbar\\\horzbar&b_2^T&\horzbar\\&\vdots&\\\horzbar&b_n^T&\horzbar} = \sum_{i=1}^n a_ib_i^\T = \m{a_i^\T b_1&a_2^\T b_2&\dots&a_1^\T b_p\\a_2^\T b_1& a_2^\T b_2i& \dots& a_2^\T b_p\\ \vdots & \vdots & \ddots & \vdots\\ a_m^\T b_1 & a_m^\T b_2 & \hdots & a_m^\T b_p},
\]
either an arrangement of every possible inner product between the rows of $A$
and the columns of $B$, or a sum of outer products between the columns of $A$
and the rows of $B$.

\subsection{Properties of Matrix Multiplication}

\begin{enumerate}[-]
    \item Matrix multiplication is associative: $(AB)C = A(BC)$
    \item Matrix multiplication is distributive: $A(B+C) = AB + AC$
    \item Matrix multiplication is not commutative \textit{in general}:
        $AB \neq BA$ (in the vast majority of cases).
\end{enumerate}

\section{Operations and Properties}

Most of this should hopefully be review, but if you haven't seen complex matrix operations
before don't worry we won't use them much.

\subsection{The Identity Matrix and Diagonal Matrices}

The \textit{identity matrix}, denoted $I\in\RR^{n\times n}$, is a square matrix
with ones on the diagonal and zeros everywhere else:
\[
    I_{ij} = \begin{cases} 1 & \text{if $i=j$}\\
        0 & \text{otherwise.}
    \end{cases}
\]
For any $A\in\RR^{m\times n}$,
\[
    AI = A = IA.
\]
Intuitively, this means that as an operator the identity matrix maps every
vector to itself.\\

A \textit{diagonal matrix} is a matrix where all non-diagonal elements are 0.
This is denoted $D=\mathrm{diag}(d_1,d_2,\dots,d_n)$ where
\[
    D_{ij} = \begin{cases}
        d_i & \text{if $i=j$}\\
        0 & \text{otherwise.}
    \end{cases}
\]

\subsection{The Transpose: $A^\T$}

The \textit{transpose} of a matrix is where every row becomes a column. Given
$A\in\RR^{m\times n}$, the transpose of $A$, $A^\T\in\RR^{n\times m}$ is defined
as
\[
    (A^\T)_{ij} = A_{ji}.\\
\]

Here are some helpful properties (proofs are easily verifiable):
\begin{enumerate}[-]
\item $(A^\T)^\T = A$
\item $(AB)^\T = B^\T A^\T$
\item $(A+B)^\T = A^\T + B^\T$
\end{enumerate}

\subsection{Symmetric Matrices}

A matrix $S\in\RR^{n\times n}$ is \textit{symmetric} if and only if
$S = S^\T$. For any $A\in\RR^{n\times n}$, $A+A^\T$ and $A^\T A$
are both symmetric. This is easily verified from the above properties.
We denote the set of symmetric matrices in $\RR^{n\times n}$ as $\SS^n$.
As we will see, symmetric matrices are often nice to work with.

\subsection{The Conjugate Transpose: $A^\dagger$}

For complex matrices $A\in\CC^{m\times n}$, the conjugate transpose of
$A$, said ``$A$ Hermitian'' is denoted as $A^\dagger = \overline{A^\T}$.
That is,
\[
    (A^\dagger)_{ij} = \overline{A}_{ji}.
\]
For example,
\[
    \m{1+2\ii & 3\ii\\ 1 & 2-\ii}^\dagger = \m{1-2\ii&1\\-3\ii&2+\ii}
\]

\subsection{Hermitian Matrices}

A matrix is Hermitian if $A = A^\dagger$. This is a generalization of symmetric
matrices from real to complex matrices.

\subsection{The Trace $\tr(A)$}

The trace of a square matrix $A\in\RR^{n\times n}$ is the sum of the diagonal
elements:
\[
    \tr A = \tr(A) = \sum_{i=1}^n A_{ii}.
\]
Here are some useful properties:
\begin{enumerate}[-]
\item For $A\in\RR^{n\times n}$, $\tr A = \tr A^\T$
\item For $A,B\in\RR^{n\times n}$, $\tr(A+B) = \tr A + \tr B$
\item For $A,B$ such that $AB$ is square, $\tr AB = \tr BA$
\item For $A,B,C$ such that $ABC$ is square, $\tr ABC = \tr BCA
    = \tr CAB$, and this can be extended to more matrices.
\end{enumerate}

(from CS229) Here's a proof of the fourth property:
\begin{align*}
    \tr AB &= \sum_{i=1}^m(AB)_{ii}\\
    &= \sum_{i=1}^m\sum_{j=1} A_{ij}B_{ij}\\
    &= \sum_{j=1}^n\sum_{i=1}^m B_{ij}A_{ij} = \sum_{j=1}^n (BA)_{ii}\\
    &= \tr BA,
\end{align*}
as desired.

A very useful property is that the trace of a matrix is the sum
of the eigenvalues of that matrix, as we will see.

\subsection{Norms}

A \textit{norm} of a vector $||\xx||$ is to an approximation a measure of
length of $\xx$. We define the $\ell-p$ norm as
\[
    ||\xx||_p = \left( \sum_{i=1}^n x_i^p \right)^{1/p}.
\]
The \textit{distance} between a two vectors is in an $\ell-p$ space
is $||\xx-\yy||_p$. Euclidean distance, $||\xx-\yy||_2$ is what we normally
consider as `distance'.

There are really 4 cases of the $\ell-p$ norm that we might encounter:
\begin{enumerate}[-]
    \item $||\xx||_0 = $(the number of non-zero elements of $\xx$)
    \item $||\xx||_1 = \sum |\xx_i|$. We call $||\xx-\yy||_1$ the \textit{Manhatten Distance}
        between $\xx$ and $\yy$ because it treats walking between coordinates in
        $\RR^n$ as walking on perpendicular streets. This is opposed to Euclidean Distance
        where you can walk in a strait line from point to point.
    \item $||\xx||_2 = \sqrt{\sum\xx_i}$ is the Euclidean Norm. This is what we normally
        consider as the `length' of a vector.
    \item $||\xx||_\infty = \max_i |x_i|$.
\end{enumerate}
We will encounter $\ell-p$ norms \textit{a lot} in our studies. Specifically, we can
improve the robustness of many algorithms to outliers by constraining the norm
of some parameter in our optimization. Infinitely many other norms can be defined,
however. A norm is \textit{any} function $f : \RR^n \mapsto \RR$ that satisfies 4
properties:
\begin{enumerate}
\item For all $\xx\in\RR^n$, $f(\xx)\geq 0$ \hfill (non-negativity)
\item $f(\xx)=0$ if and only if $\xx=\0$ \hfill (definiteness)
\item For all $\xx\in\RR^n$ and $t\in\RR$, $f(t\xx) = |t|f(\xx)$ \hfill (homogeneity)
\item For all $\xx,\yy\in\RR^n$, $f(\xx+\yy) \leq f(\xx) + f(\yy)$ \hfill (triangle inequality)\\
\end{enumerate}

Norms can be extended to matrices too, as we will encounter. Here are
a few, where $A\in\RR^{m\times n}$:
\begin{enumerate}
    \item $||A||_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n A_{ij}^2} = \sqrt{\tr(A^\T A)}$ is the Frobenius Norm of $A$.
    \item $||A||_* = \tr(\sqrt{A^\dagger A}) = \sum_{i=1}^{\min\{m,n\}}\sigma_i = $(sum of the singular values of $A$) is the Nuclear Norm
    \item $||A||_{\mathrm{max}} = \max_{ij}|A_{ij}|$
    \item $||A||_2 = \sqrt{\lambda_{max}(A^\dagger A)} = \sigma_{max}(A)$ is the Spectral Norm.
\end{enumerate}

\subsection{Linear Independence, Span and Rank}

A \textit{linear combination} of the vectors $\xx_1,\xx_2,\dots,\xx_n$ is a sum
of scalar multiples of each of the vectors. That is, for $\alpha_i\in\RR$,
\[
    \alpha_1\xx_1 + \alpha_2\xx_2 + \dots + \alpha_n\xx_n
\]
is a linear combination of those vectors, for any $\alpha_i$.\\

The \textit{span} of a set of vectors $X = \{\xx_1,\xx_2,\dots,\xx_n\} \subset \RR^m$
is every possible linear combination of $X$:
\[
    \mathrm{span}\{\xx_1,\xx_2,\dots,\xx_n\} = \alpha_1\xx_1 + \alpha_2\xx_2 + \dots + \alpha_n\xx_n
\]
for any $\alpha_1,\alpha_2,\dots,\alpha_n\in\RR$.\\

A set of vectors $\{\xx_1,\xx_2,\dots,\xx_n\} \subset \RR^m$ is
\textit{linearly independent} if no vector can be represented as
a linear combination of the others. If some
\[
    \xx_i \in \mathrm{span}\{\xx_1,\xx_2,\dots, \xx_{i-1}, \xx_{i+1},\dots,\xx_n\},
\]
the vectors are said to be \textit{linearly dependent}. As an example,
the vectors
\[
    \xx_1 = \m{1\\1\\1} \qquad \xx_2 = \m{1\\2\\3} \qquad \xx_3 = \m{0\\1\\2}
\]
are linearly dependent because $\xx_3 = \xx_2-\xx_1$.\\

The \textit{rank} of a matrix $A\in\RR^{m\times n}$ is cardinality (size) of the largest set of
columns in $A$ that are linearly independent (this is also called the \textit{column rank} of $A$).
Coincidentally, this is also the cardinality of the largest set of \textit{rows} of $A$ that are 
linearly independent (this is also called the \textit{row rank} of $A$). Here are some helpful
properties of rank:
\begin{enumerate}
\item For $A\in\RR^{m\times n}$, $\rank(A)\leq\min(m,n)$. If $\rank(A)=\min(m,n)$, then $A$
    is said to be \textit{full rank}.
\item For $A\in\RR^{m\times n}$, $\rank(A) = \rank(A^\T)$.
\item For $A\in\RR^{m\times n}$, $B\in\RR^{n\times p}$, $\rank(AB)\leq \min(\rank(A),\rank(B))$.
\item For $A,B\in\RR^{m\times n}$, $\rank(A+B)\leq \rank(A) + \rank(B)$.
\end{enumerate}

\subsection{The Inverse $A^{-1}$}

The \textit{inverse} of a matrix $A\in\RR^{n\times n}$, denoted $A^{-1}$, is the unique matrix
such that
\[
    A^{-1}A = I = AA^{-1}.
\]
If we think of $A$ as a linear map from vectors $V$ in $\RR^n$ to different vectors $W$ in $\RR^n$
such that $A : V \mapsto W$, the inverse $A^{-1}$ is the linear map (matrix) that maps vectors in
$W$ back to $V$ such that $A^{-1}: W \mapsto V$. Then $AA^{-1}:V\mapsto W\mapsto V = I$.
Note that this is an abuse of notation because
$V=W$ and the $\mapsto$ does not imply a bijective mapping, but nevertheless it should convey the
idea more intuitively.\\

Using this intuition, it should be clear that a matrix $A\in\RR^{m\times n}$, which maps $A: \RR^n
\mapsto \RR^m$ can not be invertible if $m\neq n$ because (without loss of generality suppose $m < n$)
then you are effectively `losing information' by projecting all of $\RR^n$ into a smaller space which
you cannot recover.\\

To that end, a matrix $A\in\RR^{n\times n}$ is invertible if and only if $\rank(A) = n$. A matrix that
is non-invertible is called \textit{singular}. Here are some helpful properties:
\begin{enumerate}
\item $(A^{-1})^{-1} = A$
\item $(AB)^{-1} = B^{-1}A^{-1}$
\item $(A^{-1})^\T = (A^\T)^{-1}$. We often denote $(A^{-1})^\T$ as $A^{-\T}$ because
    of this fact.\\
\end{enumerate}

Given a linear system $A\xx=\bb$, if $A$ is invertible we can multiply on
the left by $A^{-1}$, giving
\[
    A^{-1}A\xx = I\xx = \xx = A^{-1}\bb,
\]
a closed form for $\xx$. For the love of all that is holy, however, \textit{please} do
not solve things numerically by calculating the inverse of any matrices!
\footnote{at the very least instead of $A^{-1}\bb$ (\texttt{pinv(A)*b} Matlab or Julia) use $A\setminus\bb$
(\texttt{A$\backslash$b} in Matlab or Julia).}

\end{document}
