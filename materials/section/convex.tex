\documentclass{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate,mathtools}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[letterpaper,margin=1in]{geometry}

\makeatletter
\let\Hy@linktoc\Hy@linktoc@none
\makeatother

\input{macros.tex}

\setlength{\parindent}{0pt}

\begin{document}

\begin{center}
  \Large\textbf{Convex Optimization Overview}\\
  \large\textit{Conner DiPaolo}
\end{center}
\vspace*{1em}

\tableofcontents
\vspace{1em}

\section{Introduction}

Convex optimization in a large way influences the way
people think about and phrase machine learning problems.
Almost all problems we will see in our studies are developed
or can be viewed as optimization problems. Some problems,
like the Support Vector Machine you will all see in the coming
weeks, are almost entirely based in the heart of convex optimization.\\

We don't plan on bringing you all up to speed completely on
the art of Convex Optimization, but hopefully in two sections you
will know enough to be able to think about problems in new, interesting
ways that will aid your studies in and out of machine learning.\\

The only real prerequisite for this material is a strong confidence
in linear algebra and familiarity with matrix calculus. Note that
much of this material stems from Boyd and Vandenberghe's insanely influential
textbook \textit{Convex Optimization}. If you are interested in the
topic or need a more in-depth resource, check out the book. It's free
online\footnote{\url{http://stanford.edu/~boyd/cvxbook/}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convex Sets}



The first step into examining convexity is defining what a
\textbf{convex set} is when given, for example, a subset of the real numbers,
or the set of matrices.

\begin{figure}
    \centering
    \begin{center}
        \begin{tikzpicture}
            \draw[fill=black] (0,0) circle (0.05) node[above] {$\xx$};
            \draw[fill=black] (4,0) circle (0.05) node[above] {$\yy$};
            \draw (0,0) -- (4,0);
            \draw (2,0.25) node {$\theta\xx + (1-\theta)\yy$};
        \end{tikzpicture}
    \end{center}
    \caption{\textbf{Convex Combination}. The line segment between $\xx$ and $\yy$ 
    above represents every possible combination $\{\theta\xx + (1-\theta)\yy : 
    0\leq\theta\leq1\}$.}
    \label{fig:convex-comb}
\end{figure}

\begin{definition}[Convex Combination]
    In the $n=2$ case, the convex combination of points $\xx$ and $\yy$ in an
    affine space is
    \[
        \theta\xx + (1-\theta)\yy
    \]
    where $0\leq\theta\leq1$. Intuitively, this is the line segment between
    $\xx$ and $\yy$ (consider $\theta=0$ and $\theta=1$), as seen in Figure
    \ref{fig:convex-comb}. More generally,
    a convex combination of points $\xx_1,\xx_2,\dots,\xx_n$ in an affine space (vector
    spaces included) is the combination
    \[
        \thetab_1\xx_1 + \thetab_2\xx_2 + \dots + \thetab_n\xx_n = \thetab^\T\xx
    \]
    where $\theta_i \geq 0$ and $\1^\T\theta = 1$. That is, $\thetab$ lies on the
    standard probability simplex.  
\end{definition}

With this definition of a convex combination we can define
a convex set:

\begin{figure}
    \centering
    \begin{center}
        \begin{tikzpicture}
            \begin{scope}[shift={(-3,0)}]
                \draw (0,3.5) node {\textbf{Convex Set}};
                \fill[color=green!30] (0,0) -- (-1,2) -- (0,3) -- (1,2.5) -- (2,0)  -- cycle;
                \draw[very thick] (-0.5,2) -- (1,0.5);
            \end{scope}
            \begin{scope}[shift={(3,0)}]
                \draw (1,3.5) node {\textbf{Non-Convex Set}};
                \fill[color=red!30] plot [smooth cycle] coordinates {(0,1) (1,2) (2,2) (1,1) (2,0) (0,0)};
                \draw[very thick] (1.4,1.75) -- (1.2,0.5);
            \end{scope}
        \end{tikzpicture}
    \end{center}
    \caption{{\bf Set convexity.} Intuitively, a set is convex
    if a line drawn between any two points within the set lies completely in the set. The convex set
    to the left is a \textit{convex hull} of it's vertices, meaning the set is constructed
    as all possible convex combinations of its vertices. These sets are subsets of $\RR^2$.}
    \label{fig:convex-sets}
\end{figure}

\begin{definition}[Convex Set]
    A set $C$ is convex if, given $\xx,\yy \in C$, every convex combination
    of $\xx$ and $\yy$ is still in $C$. Mathematically,
    \[
        \theta\xx + (1-\theta)\yy \in C. \tag{$0\leq\theta\leq1$}
    \]
    See Figure \ref{fig:convex-sets} for an illustration. Intuitively, this
    means that every line segment between any two points in $C$ is contained
    entirely within $C$.
\end{definition}

\subsection{Examples of Convex and Non-Convex Sets}

\begin{example}[All of $\RR^n$]
    $\RR^n$ is convex.
\end{example}
\begin{proof}
    As a vector space, for any $\theta_1,\theta_2 \in \RR$ and $\xx$ and
    $\yy$ in $\RR^n$,
    \[
        \theta_1 \xx + \theta_2 \yy \in \RR^n.
    \]
    Thus, restricting $\theta_2 = 1-\theta_1$ and $0\leq\theta_1\leq1$
    does not change this fact, and any convex combination is also in
    $\RR^n$, making the set convex by definition.
\end{proof}

\begin{example}[The Non-Negative Orthant $\RR^n_+$]
    The set of all vectors
    \[
        \{\xx : \xx\in\RR^n \text{ and } \xx_i \geq 0\}
    \]
    is convex.
\end{example}
\begin{proof}
    Left as an exercise to the reader.
\end{proof}

\begin{example}[Closed Intervals in $\RR$]
    \label{ex:closed-intervals-convex}
    Let $C = [a,b]$ be a subset of the real numbers where $a \leq b$.
    Then $C$ is convex.
\end{example}
\begin{proof}
    Suppose, without loss of generality, that $x_1 \leq x_2$ where
    $x_1,x_2 \in [a,b]$. Now let $0\leq\theta\leq1$. Then
    \[
        \theta x_1 + (1-\theta) x_2 \leq \theta x_2 + (1-\theta) x_2 = x_2
    \]
    because $\theta x_1 \leq \theta x_2$. Similarly,
    \[
        \theta x_1 + (1-\theta) x_2 \geq \theta x_1 + (1-\theta) x_1 = x_1
    \]
    because $(1-\theta) x_2 \geq (1-\theta) x_1$. Thus
    \[
        a \leq x_1 \leq \theta x_1 + (1-\theta) x_2 \leq x_2 \leq b,
    \]
    and hence
    \[
        \theta x_1 + (1-\theta) x_2 \in [a,b].
    \]
    Therefore, by the definition of convexity, $[a,b]\subseteq\RR$ is convex
    for any $a \leq b$.
\end{proof}

\begin{example}[The Set of All Complex Hermetian Matrices]
    \label{ex:hermitian-convex}
    Let $C = \{A : A\in\CC^{n\times n} \text{ and } A^* = A\}$. $C$ is
    convex.
\end{example}
\begin{proof}
    Let $A,B\in\CC^{n\times n}$ be Hermitian matrices and $0\leq\theta\leq1$.
    Then
    \begin{align}
        (\theta A + (1-\theta) B)^* &= (\theta A)^* + \left[(1-\theta)B\right]^*\\
        &= \theta A^* + (1-\theta) B^*\\
        &= \theta A + (1-\theta) B \tag{because $A^*=A$ and $B^*=B$}\\
    \end{align}
    Thus every convex combination of Hermitian matrices is Hermitian,
    and by the definition of convexity the set is convex.
\end{proof}

\begin{example}[The Set of All Real Symmetric Matrices]
    Let $C = \{A : A\in\RR^{n\times n} \text{ and } A^\T = A\}$. $C$ is
    convex.
\end{example}
\begin{proof}
    Left as an exercise to the reader.
\end{proof}

\begin{example}[The Set of All Linear Matrix Inequalities]
    Let $A_i$ and $B$ be symmetric $n\times n$ matrices and
    $\xx\in\RR^n$.
    Let $C = \{\xx : A(\xx) \preceq B\}$ where $A(\xx) = \xx_1A_1 +
    \dots + \xx_kA_k$. $C$ is convex.
\end{example}
\begin{proof}
    Let $0\leq\theta\leq1$ and $\xx, \yy \in C$. Then
    \begin{align*}
        A(\theta\xx + (1-\theta)\yy) &= \sum_i \left[ \theta\xx_i + (1-\theta)\yy_i \right]A_i\\
        &= \theta\sum_i \xx_i A_i + (1-\theta)\sum_i \xx_i A_i\\
        &= \theta A(\xx) + (1-\theta) A(\yy)\\
        &\leq \theta B + (1-\theta) B\\
        &= B.
    \end{align*}
    Thus any convex combination of elements of $C$ is contained
    in $C$ and by definition $C$ is convex.
\end{proof}

\begin{example}[The Space of Probability Distributions]
    Let $\Pc$ be the space of continuous probability distributions over
    $\RR^n$. That is, every element of $\Pc$ defines a unique probability
    density function $\PP(x) \geq 0$ such that \[\int_{\RR^n} \PP(x) dx=1.\]
    $\Pc$ is convex.
\end{example}
\begin{proof}
    Let $f$ and $h$ be valid probability distributions from $\Pc$. That is,
    \[
        f,h \in \left\{\PP(x) : \int_{\RR^n} \PP(x)dx = 1 \text{ and } \PP(x) \geq 0 \right\}.
    \]
    Now let $0\leq\theta\leq1$. Then
    \[
        \theta f(x) + (1-\theta) h(x) \geq 0
    \]
    as a positive combination of positive functions. Similarly,
    \begin{align*}
        \int_{\RR^n}\left[ \theta f(x) + (1-\theta) h(x) \right]dx &= \theta\int_{\RR^n}f(x)dx + (1-\theta)\int_{\RR^n}h(x)dx\\
        &= \theta + (1-\theta) = 1.
    \end{align*}
    Thus every convex combination of probability distributions over $\RR^n$ is
    a valid distribution (often called a mixture), and therefore the set of
    all valid probability distributions over $\RR^n$ is convex itself.
\end{proof}

\begin{example}[Disjoint Intervals in $\RR$]
    Let $N = [a,b] \cup [c,d]$ where $a\leq b < c \leq d$.
    $N$ is \textbf{not} convex.
\end{example}
\begin{proof}
    Let $b,c\in N$ be as described above. Then for $0<\theta<1$
    (not we aren't including inequality),
    \[
        \theta b + (1-\theta)c \not\in N.
    \]
    Thus not \textit{every} convex combination of elements in
    $N$ is in $N$, and $N$ is not convex.
\end{proof}

\begin{example}[The Set of All Stochastic Matrices]
    The set of all matrices $A$ such that for all elements $0 \leq A_{ij} \leq 1$
    and each row sums to $1$,
    \[
        M = \{A : A\in\RR^{m\times n} \text{ and } 0\leq A_{ij}\leq1 \text{ and } A\1 = \1\},
    \]
    is convex.\\

    Note that this set is the set of all matrix representations of every possible
    Markov Chain.
\end{example}
\begin{proof}
    Let $A,B\in M$ and $0\leq\theta\leq1$. Then
    \[
        c = \left[ \theta A + (1-\theta)B \right]_{ij} = \theta A_{ij} + (1-\theta)B_{ij}
    \]
    satisfies $0\leq c\leq 1$ as $A_{ij},B_{ij}\in [0,1]$ and closed intervals
    on $\RR$ are convex as seen in Example \ref{ex:closed-intervals-convex}.\\

    Further, because $A\1=\1$ and $B\1=\1$,
    \[
        \left[\theta A + (1-\theta)B\right]\1 = \theta A\1 + (1-\theta)B\1 = \theta\1 + (1-\theta)\1 = \1,
    \]
    as desired. Thus every convex combination of elements withn $M$ remains in
    $M$, and by definition the set $M$ is convex.
\end{proof}

\begin{example}[Norm Balls]
    For any valid norm $||\cdot||$ and scalar $r \geq 0$, the set
    \[
        C = \{\xx : ||\xx|| \leq r\}
    \]
    is convex.
\end{example}
\begin{proof}
     Let $\xx$ and $\yy$ be elements from $C$ and $0\leq\theta\leq1$. Then
     \[
         ||\theta\xx + (1-\theta)\yy|| \leq ||\theta\xx|| + ||(1-\theta)\yy|| = \theta||\xx|| + (1-\theta)||\yy|| \leq \theta r + (1-\theta)r = r,
     \]
     as desired, where we used the Triangle Inequality for the first step
     and the second used the homogeneity of norms. Thus every convex combination
     of elements in $C$ is within $C$, and therefore $C$ is convex.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convex Functions}

You likely have already seen convex functions from your time in Calculus I in high
school or something similar. Nevertheless, our treatment will be much more rigorous
(though not \textit{too} rigorous) and be very applicable to our later studies.\\

The most important fact to know about convex functions is that they
\begin{enumerate}[(a)]
    \item Every local minimum is a global minimum
    \item Generally have efficient algorithms for finding such a minimizer.
\end{enumerate}

\subsection{Convex and Concave Functions}

Our idea of a convex set will be useful in considering convex functions.

\begin{figure}
    \centering
    \begin{center}
        \begin{tikzpicture}
            \begin{scope}[shift={(-3,0)}]
                \draw[->] (-2.2,0) -- (3.5,0) node[right] {$x$};
                \draw[->] (0,-0.2) -- (0,4) node[above] {$y$};
                \draw[domain=-2:3.3,
                      smooth,
                      variable=\x,
                      blue] plot ({\x},{\x*\x/4});
                \fill[color=white] (-1,2) rectangle (2.75,2.5);
                \draw[fill=black] (-1,0.25) circle (0.05) node[above]{$f(\xx)$};
                \draw[fill=black] (3,2.25) circle (0.05) node[right]{$f(\yy)$};
                \draw (-1,-0.2) node {$\xx$};
                \draw (3,-0.2) node {$\yy$};
                \draw[thick] (-1,0.25) -- (3,2.25);
                \draw (1,2.25) node {$\theta f(\xx) + (1-\theta)f(\yy)$};
                \draw (1,-0.75) node {$\theta \xx + (1-\theta)\yy$};
                \draw (-1,-0.75) -- ++(0,0.1) -- ++(0,-0.2) -- ++(0,0.1) -- ++(0.75,0);
                \draw (3,-0.75) -- ++(0,0.1) -- ++(0,-0.2) -- ++(0,0.1) -- ++(-0.75,0);
            \end{scope}

            \begin{scope}[shift={(-3,0)}]
            \end{scope}
        \end{tikzpicture}
    \end{center}
    \caption{\textbf{Convex Functions}. This function $f : [a,b] \mapsto \RR$ is convex
    because it's domain is convex and every line between two points on the function lies
    above the function.}
    \label{fig:convex-function}
\end{figure}

\begin{definition}[Convex]
    Given a convex set $C$, a function $f : C \mapsto \RR$ is convex if,
    for $0\leq\theta\leq1$, given any $\xx,\yy\in C$,
    \[
        f(\theta\xx + (1-\theta)\yy) \leq \theta f(\xx) + (1-\theta)f(\yy).
    \]
\end{definition}

Intuitively, this means that a convex function always lies under a line
between any two points where it is evaluated. This is seen in Figure \ref{fig:convex-function}.

\begin{definition}[Concave Functions]
    Given a convex set $C$, function $f : C \mapsto \RR$ is concave
    if $-f$ is convex. That is, for $0\leq\theta\leq1$, given any $\xx,\yy\in C$,
    \[
        f(\theta\xx + (1-\theta)\yy) \geq \theta f(\xx) + (1-\theta)f(\yy).
    \]
\end{definition}

\begin{theorem}[Restriction To a Line]
    A function $f : \RR^n \mapsto \RR$ is convex if and only
    if $f$ is convex when restricted to any line that intersects
    its domain. Mathematically, $f$ is convex if and only if
    for all $x\in C$ and any $\vv\in\RR^n$ and $t\in\RR$,
    \[
        g(t) = f(\xx + t\vv)
    \]
    is convex where $g : \{t : \xx + t\vv \in C\} \mapsto \RR$
\end{theorem}
\begin{proof}
    Omitted. See \textit{Convex Optimization} by Boyd and Vandenberghe.
\end{proof}

\subsection{First Order Conditions: $f(x) \geq f(y) + \nabla f(y)^\T (x-y)$}

Generally speaking, determining convexity from the definition is
hard. In this section we will develop more tractable conditions that
will help us determine if an arbitrary function is convex. The following
necessary and sufficient condition for convexity is called the First Order
Condition because it relies only on the first derivative.

\begin{theorem}[First Order Conditions]
    Let $f$ be a function mapping from some convex set $C$ to $\RR$. Then
    $f$ is convex if and only if, given $\xx$ and $\yy$ from $C$,
    \[
        f(\xx) \geq f(\yy) + \nabla f(\yy)^\T (\xx - \yy).
    \]
\end{theorem}
\begin{proof}
    Omitted. See \textit{Convex Optimization} by Boyd and Vandenberghe.
\end{proof}

\subsubsection{Examples On Determining Convexity}

\begin{theorem}[Linear Functions are Both Concave and Convex]
    Given a function $f : C \mapsto \RR$ where $C$ is a convex
    set and for any $a,b\in\RR$ and $\xx,\yy\in\RR$
    \[
        f(a\xx + b\yy) = af(\xx) + bf(\yy),
    \]
    $f$ is both convex and concave. Note that $f$ is the definition
    of a linear function.
\end{theorem}
\begin{proof}
    Let $\xx,\yy$ be any elements of $C$ and $0\leq\theta\leq1$. Then
    by linearity
    \[
        f(\theta\xx + (1-\theta)\yy) = \theta f(\xx) + (1-\theta)f(\yy).
    \]
    Thus by definition $f$ is both convex and concave.
\end{proof}

\begin{example}
    $f(x) = x^2$ is convex.
\end{example}
\begin{proof}
    We will use the much more convenient second order condition
    for this problem later. Consider $x,y\in\RR$ and $0\leq\theta\leq1$.
    Then
    \begin{align*}
        f(\theta x + (1-\theta)y) &= (\theta x + (1-\theta)y)^2\\
        &= (\theta x + (1-\theta)y)(\theta x + (1-\theta)y)\\
        &= \theta^2x^2 + 2\theta(1-\theta)xy + (1-\theta)^2y^2.
    \end{align*}
    $f$ will be convex if and only if
    \[
        \theta f(x) + (1-\theta)f(y) - f(\theta x + (1-\theta)y) \geq 0
    \]
    for all $x,y$. We have
    \begin{align*}
        g = \theta f(x) + (1-\theta)f(y) - f(\theta x + (1-\theta)y) &= \theta x^2 + (1-\theta)y^2 - \theta^2x^2 - 2\theta(1-\theta)xy - (1-\theta)^2y^2\\
        &= \theta(1-\theta)x^2 - 2\theta(1-\theta)xy + \theta(1-\theta)y^2\\
        &= \theta(1-\theta)(x-y)^2
    \end{align*}
    We know $(x-y)^2 \geq 0$ for all $x$ and $y$. Similarly, when
    $\theta \in [0,1]$, both $\theta$ and $1-\theta$ are positive,
    so this expression is positive. Thus $f$ must be convex, as desired.
\end{proof}

\subsection{Second Order Conditions: $\nabla^2 f \succeq 0$}

Here we discuss the most useful condition for determining convexity.
\begin{theorem}[Second Order Conditions]
    A twice-differentiable function $f : C \mapsto \RR$ is convex if and
    only if and only if the Hessian
    \[
        \nabla^2 f \succeq 0
    \]
\end{theorem}
\begin{proof}
    Omitted.
\end{proof}

\subsubsection{More Examples On Determining Convexity}

\begin{example}[Quadratic Functions]
    The function $f : \RR^n \mapsto \RR$ defined by
    \[
        f(\xx) = \xx^\T A\xx + \bb^\T\xx + c
    \]
    is convex if $A \succeq 0$.
\end{example}
\begin{proof}
    We know
    \[
        \nabla^2 f = 2A.
    \]
    Similarly we know that $2A \succeq 0$ if and only if
    $A \succeq 0$. Thus by the second order conditions $f$
    is convex if and only if $A \succeq 0$.\\

    We can also show that $f$ is concave if $A\preceq 0$.
\end{proof}

\begin{example}
    $f(x) = a\;e^x$ is convex if $a \geq 0$.
\end{example}
\begin{proof}
    The Hessian
    \[
        \nabla^2f = \frac{d^2f}{dx^2} = a\;e^x \geq 0
    \]
    as long as $a\geq0$. Thus by the second order conditions
    $f$ is convex if $a\geq0$.
\end{proof}

\begin{example}
    The function $f : \RR^n \mapsto \RR$ defined by
    \[
        f(\xx) = e^{\xx^\T\xx} = e^{\xx_1^2 + \dots + \xx_n^2}
    \]
    is convex.
\end{example}
\begin{proof}
    Each element of the Hessian
    \[
        \nabla^2f_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j} = 4\xx_i\xx_j e^{\xx^\T\xx}.
    \]
    Therefore
    \[
        \nabla^2f = 4\xx\xx^\T e^{\xx^\T\xx}.
    \]
    Is this positive semidefinite? Consider $\zz\in\RR^n$. Then
    \[
        \zz^\T\nabla^2 f(x)\zz = \zz^\T4\xx\xx^\T e^{\xx^\T\xx} \zz = 4(\zz^\T\xx)^2 e^{\xx^\T\xx} \geq 0
    \]
    because $\exp(\cdot)$ and $(\zz^\T\xx)^2$ are both non-negative.
    Thus $\nabla^2f \succeq 0$ and by the second order conditions
    $f$ is convex.
\end{proof}

\subsection{Operations Preserving Convexity}

In this section we examine some handy tools for constructing
convex functions from other convex functions. There are \textit{many}
more properties than those shown here. See \textit{Convex Optimization}
by Boyd and Vandenberghe for many others.

\begin{theorem}[Non-Negative Weighted Sum]
    The function $F : C \mapsto \RR$ where $C$ is a convex set, $f_i : C\mapsto\RR$ is a convex
    function, $w_i \geq 0$ and
    \[
        F(\xx) = \sum_i w_i f_i(\xx)
    \]
    is convex.
\end{theorem}
\begin{proof}
    Let $\xx,\yy\in C$ and $0\leq\theta\leq1$. Then
    \begin{align*}
        F(\theta\xx + (1-\theta)\yy) &= \sum_i w_if_i(\theta\xx + (1-\theta)\yy)\\
        &\leq \sum_i w_i\left[ \theta f_i(\xx) + (1-\theta)f_i(\yy) \right]\\
        &= \theta F(\xx) + (1-\theta)F(\yy),
    \end{align*}
    as desired.
\end{proof}

\begin{theorem}[Pointwise Maximum]
    Given convex functions $f_i : C \mapsto \RR$, and $F : C \mapsto \RR$ defined
    as
    \[
        F(\xx) = \max_i f_i(\xx),
    \]
    $F$ is convex.
\end{theorem}
\begin{proof}
    By the definition of convexity, given $\xx,\yy\in C$ and $0\leq\theta\leq1$ we
    have
    \begin{align*}
        F(\theta\xx + (1-\theta)\yy) &= \max_i f_i(\theta\xx + (1-\theta)\yy)\\
        &\leq \max_i\{\theta f_i(\xx) + (1-\theta)f_i(\yy)\} \tag{because $f_i$ is convex}\\
        &\leq \theta \max_i f_i(\xx) + (1-\theta)\max_i f_i(\yy)\\
        &= \theta F(\xx) + (1-\theta) F(\yy),
    \end{align*}
    as desired.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimization Problems}

An optimization problem is a problem of the form
\begin{align*}
    \text{minimize: } & f_0(\xx)\\
    \text{subj. to: } & f_i(\xx) \leq 0, \quad i=1,\dots,m\\
                      & h_i(\xx) = 0, \quad i=1,\dots,p
\end{align*}
where $f_0 : \RR^n \mapsto \RR$ is called the \textbf{objective function},
$f_i : \RR^n \mapsto \RR$ is an \textbf{inequality constraint}, and
$h_i : \RR^n \mapsto \RR$ is an \textbf{equality constraint}. The goal of
an optimization problem, as you might be able to guess, is to find $\xx$ in
the problem domain domain 
\[
    \Dc = \bigcap_{i=0}^m\mathbf{dom}\; f_i \;\cap\; \bigcap_{i=1}^p \mathbf{dom}\; h_i
\]
that minimizes $f_0(\xx)$ subject to the given conditions. The problem is
said to be \textit{feasible} if such an $\xx$ exists, and \textit{infeasible}
otherwise.\\

The optimal value $p^\star$ of the problem above is defined to be
\[
    p^\star = \inf\{f_0(\xx) : f_i(\xx) \leq 0,\; i=1,\dots,m,\; h_i(\xx)=0,\, i=1,\dots,p\}.
\]
If there are feasible points $\xx_k$ with $f_0(\xx_k)\to -\infty$ as
$k\to\infty$, then we say the problem is \textit{unbounded below}.\\

Note that we could have a similarly defined \textit{maximization} problem.
Everything is the same except for the definition of \textit{unbounded below},
obviously.

\subsection{Optimal Points}

We say $\xx^\star$ is an \textit{optimal point} (or that it solves the
above problem) if $\xx^\star$ is feasible and $f_0(\xx^\star)=p^\star$.
If an optimal point exists, we say the problem is \textit{solvable}.\\

We say a feasible point $\xx$ is \textit{locally optimal} if there exists
some $R > 0$ such that $\xx$ solves

\begin{align*}
    \text{minimize: } & f_0(\zz)\\
    \text{subj. to: } & f_i(\zz) \leq 0, \quad i=1,\dots,m\\
                      & h_i(\zz) = 0, \quad i=1,\dots,p\\
                      & || \zz - \xx ||_2 \leq R
\end{align*}
for variable $\zz$. This intuitively means that $\xx$ minimizes
$f_0$ over nearby points in the feasible set.

\subsection{Equivalent Problems}

Given an optimization problem of the form
\begin{align*}
    \text{minimize: } & f_0(\xx)\\
    \text{subj. to: } & f_i(\xx) \leq 0, \quad i=1,\dots,m\\
                      & h_i(\xx) = 0, \quad i=1,\dots,p
\end{align*}
we may want to express our problem in a cleaner or easier
to solve form. Under certain conditions changing our problem's
form will not actually change the solution. Note that, as usual,
there are a few more possible transformations that are kosher.
Check out Boyd and Vandenberghe for these, but the ones shown here
are likely the only ones you will need.

\subsubsection{Scaling}

The problem
\begin{align*}
    \text{minimize: } & \alpha_0f_0(\xx)\\
    \text{subj. to: } & \alpha_if_i(\xx) \leq 0, \quad i=1,\dots,m\\
                      & \beta_ih_i(\xx) = 0, \quad i=1,\dots,p
\end{align*}
for $\alpha > 0$ and $\beta \neq 0$ is equivalent to the original
problem. This should be intuitive since, for example $x^2$ is minimized
at 0, changing the scale of your axes maintains that minimum. Similarly,
if equality holds (ie. $4x+2=0$) multiplying by any non-zero number on
both sides maintains that equality.

\subsubsection{Change of Variables}

Given an one-to-one mapping $\phi : \RR^n \mapsto \RR^n$, where the image
(range) of $\phi$ covers the domain of your problem $\Dc$, the problem
\begin{align*}
    \text{minimize: } & f_0(\phi(\xx))\\
    \text{subj. to: } & f_i(\phi(\xx)) \leq 0, \quad i=1,\dots,m\\
                      & h_i(\phi(\xx)) = 0, \quad i=1,\dots,p
\end{align*}
is equivalent to the original. This should be clear. If $x$ solves the
original problem, then $z = \phi^{-1}(x)$ solves the transformed problem.
The converse also holds.

\subsubsection{Transformation of Objective and Constrain Functions}

Suppose $\psi_0 : \RR \mapsto \RR$ is monotonically increasing, $\phi_1,\dots,
\psi_m : \RR \mapsto \RR$ satisfy $\phi_i(u) \leq 0$ if and only if $u \leq 0$,
and $\psi_{m+1},\dots,\phi_{m+p} : \RR\mapsto\RR$ satisfy $\phi_i(u)=0$ if and only
if $u=0$. The problem
\begin{align*}
    \text{minimize: } & \psi_0(f_0(\xx))\\
    \text{subj. to: } & \psi_i(f_i(\xx)) \leq 0, \quad i=1,\dots,m\\
                      & \psi_{m+i}(h_i(\xx)) = 0, \quad i=1,\dots,p
\end{align*}
is equivalent to the original. This should be evident from the conditions
we placed on each $\psi_j$.

\subsubsection{Slack Variables}

This will come in very handy. A prudent observation is that $f_i(x)\leq 0$
if and only if for some $s_i \geq 0$, $f_i(x) + s_i = 0$. Thus the problem
\begin{align*}
    \text{minimize: } & f_0(\xx)\\
    \text{subj. to: } & f_i(\xx) + s_i = 0, \quad i=1,\dots,m\\
                      & h_i(\xx) = 0, \quad i=1,\dots,p\\
                      & s_i \geq 0, \quad i=1,\dots,m
\end{align*}
is equivalent to the original.

\subsubsection{Epigraph Form}

The \textit{epigraph form} of the original problem is
\begin{align*}
    \text{minimize: } & t\\
    \text{subj. to: } & f_0(\xx) - t \leq 0,\\
                      & f_i(\xx) \leq 0, \quad i=1,\dots,m\\
                      & h_i(\xx) = 0, \quad i=1,\dots,p
\end{align*}
where $t\in\RR$. It should be clear that this is the same as the original
problem as the first constraint can be viewed as $f_0(\xx) \leq t$. Thus
the objective must always lie under $t$ and minimizing $t$ will minimize the
highest possible value of $f_0$.

\subsection{Convex Optimization Problems}

We will now study the branch of optimization problems we will see
\textit{extremely} often in our studies. We say a problem of the
form

\begin{align*}
    \text{minimize: } & f_0(\xx)\\
    \text{subj. to: } & f_i(\xx) \leq 0, \quad i=1,\dots,m\\
                      & A\xx = \bb
\end{align*}

is a \textbf{convex optimization problem} if the objective function
and inequalities $f_i$ are convex, and the equality constraints
$h_i$ are linear. Convex problems are, as a whole, extremely \textit{nice}
in the sense that we have efficient (read `polynomial time') algorithms
for finding global optima.\\

At heart, a convex optimization problem is 
just a minimization of a convex function within a convex domain.


\subsubsection{Global Optimality of a Convex Optimum}

The reason that convex optimization problems are so nice to solve
is that \textit{any} optimum is a global optimum. In other words,
if you find some solution you find \textit{the} solution. This is
by no means the case in non-convex problems such as neural networks,
although you might use similar methods to find \textit{sufficiently
good solutions}.

\begin{theorem}[Global Optimality]
    Given a convex optimization problem, and local optimum $\xx^\star$
    such that
    \[
        f_0(\xx) = \inf\{f_0(\zz) : \zz\text{ feasible and } ||\zz-\xx||_2 \leq R\}
    \]
    for some $R > 0$. $\xx^\star$ is the global optimum.
\end{theorem}
\begin{proof}
    (Boyd)
    Suppose such a problem and local optimum. Now suppose, to the
    contrary, that $\xx$ is \textit{not} the global optimum,
    and therefore there exists some feasible $\yy$ such that $f_0(\yy)
    < f_0(\xx)$. Then $||\yy-\xx||_2 >R$ because otherwise $f_0(\xx)
    \leq f_0(\yy)$. Consider a point $\zz$ given by
    \[
        \zz = (1-\theta)\xx+ \theta\yy \quad\text{and}\quad \theta=\frac{R}{2||\yy-\xx||_2}.
    \]
    Then $||\xx-\zz||_2=R/2 < R$. By convexity of the feasible set,
    $\zz$ is feasible. By the convexity of the objective function $f_0$
    we also have
    \[
        f_0(\zz) \leq (1-\theta)f_0(\xx) + \theta f_0(\yy) < f_0(\xx),
    \]
    contradicting our assumption of local optimality. Thus $\xx$
    must be the global optimum as $R\to\infty$.
\end{proof}

\subsubsection{Linear Programs}

While we won't study Linear Programs much in the course, they
are necessary to understand and are a central tool in operations
research and graph algorithms. A linear program is a convex
optimization problem of the form
\begin{align*}
    \text{minimize: } & \cc^\T\xx\\
    \text{subj. to: } & G\xx \leq \hh,\\
                      & A\xx = \bb
\end{align*}
where $G\in\RR^{m\times n}$ and $A\in\RR^{p\times n}$. Note that
because the objective function is linear this could also have been
a maximization problem with no change to convexity. For a ton
of interesting applications, see Boyd and Vandenberghe, or consider
taking the Operations Research Course!

\subsubsection{Quadratic Programs}

Interestingly enough, of all standard convex optimization problem classes,
quadratic programs seem to come up the most in machine learning. Basically,
pay attention!\\

A quadratic program is an optimization program of the form
\begin{align*}
    \text{minimize: } & \frac{1}{2}\xx^\T P\xx + \qq^\T\xx + \rr\\
    \text{subj. to: } & G\xx \preceq\hh\\
                      & A\xx = \bb
\end{align*}
where $P\in\SS_+^n$, $G\in\RR^{m\times n}$, and $A\in\RR^{p\times n}$.
In this class of program, we are minimizing a quadratic function inside
of a polyhedron.

\subsection{Examples of Convex Optimization Problems}

Here we present some examples of applied convex optimization
problems, many of which are central to machine learning!

\subsubsection{Analytic Centering}

Consider the problem of finding the `center' of some
polygon $\{\xx : A\xx \preceq \bb\}$. We first need to denote
what we consider the optimal center. The \textit{Chebyshev Center}
of a set of points $\{\xx_i\}$ is the center of the smallest
circle that will fit around all of the points. This is easily
expressed as the following optimization problem:
\begin{align*}
    \text{minimize: } & r\\
    \text{subj. to: } & ||\xx_i - \cc||_r \leq r\\
\end{align*}
where $r$ and $\cc$ are variables. This is obviously convex.
We could set $\xx_i$ as the vertices of the polygon to then find a
center.\\

But what if we wanted to create
a \textit{unconstrained} convex optimization problem that would
given us some definition of a `center'? First note that $A\xx \leq \bb$
implies that $a_i^\T\xx \leq \bb_i$ where $a_i$ is the $i-$th row
of $A$. This condition is satisfied, obviously, when $-\log (\bb_i - a_i^\T\xx)$
exists. But more importantly, as $a_i^\T\xx$ approaches $\bb_i$, we
note that the $-\log$ term approaches infinity. This is called a
`$\log$ barrier'. Now if we combine a bunch of $\log$ barriers, say,
perhaps, along each of the edges of our polygon and ``walking down
hill'' until we find the point that minimizes all of the log barriers. This
sounds, intuitively like a good idea of a `center', and is called the
\textit{Analytic Center} of a polygon. This is also easily expressed as
a convex optimization problem, this time without constraints:
\begin{align*}
    \text{minimize: } f_0(\xx) = -\sum_{i}\log(\bb_i - a_i^\T\xx)\\
\end{align*}
Note that the gradient
\[
    \nabla f_0 = \sum_i\frac{\aa_i}{\bb_i - \aa_i^\T\xx},
\]
so if you are `far' from a barrier it doesn't effect your gradient.

\subsubsection{Logistic Regression}

Note that our notation will assume $\xx_0=1$ as a bias term.\\

Now let's reconsider logistic regression as an optimization problem.
Suppose we have a bunch of points $\xx_i$, with labels $\yy_i \in \{0,1\}$.
We want to classify some new point $\xx$.
Now suppose we generate a distribution over the labels of a given point
$\xx$ with
\[
    \PP(\yy = 1 | \xx,\thetab) = \sigma(\thetab^\T\xx) = \frac{1}{1 + e^{-\thetab^\T\xx}} \in (0,1).
\]
Intuitively, this takes regular linear regression over the binary labels
$\{0,1\}$ and squashes it into a function $f : \RR \mapsto [0,1]$ to give
us the ability to probabilistically interpret the output of our classifier.
In a frequentist setting we want to maximize the \textit{likelihood} of our data,
that is, the probability of our labelled data given our parameters (just
$\thetab$ in this case). However, because the actual likelihood will involve
products we can transform into log space (because $\log(x)$ is a monotonically
increasing function!!!) and equivalently maximize the log likelihood
\[
    \Lc(\thetab) = \sum_{i=1}^m \log \PP(\yy=\yy_i | \xx_i, \thetab) = \sum_{i=1}^m \log \begin{cases}
        \sigma(\thetab^\T\xx) & \text{if $\yy_i=1$}\\
        1-\sigma(\thetab^\T\xx) & \text{otherwise}
    \end{cases}
\]
Note that due to the binary labels we can represent this compactly as
\[
    \Lc(\thetab) = \sum_{i=1}^m \yy_i\log(\sigma(\thetab^\T\xx)) + (1-\yy_i)\log(1-\sigma(\thetab^\T\xx)).
\]
Now our (unconstrained) optimization problem becomes simply
\begin{align*}
    \text{maximize: } \Lc(\thetab)
\end{align*}
where we could add a $\lambda ||\thetab||_2$ regularization term if we place
a Gaussian prior on our parameter vector $\thetab$. Usually this would be a
good idea.\\

Now to optimize this, as we will/have seen, first note that the derivative of
the sigmoid function $\sigma'(x) = \sigma(x)\left(1-\sigma(x)\right)$. Then the
gradient of our log output $\log\PP(\yy=1|\xx,\thetab) = \log\sigma(\thetab^\T\xx)$
becomes
\[
    \nabla_\theta \log\sigma(\thetab^\T\xx) = \xx(1-\sigma(\thetab^\T\xx) ).
\]
Similarly,
\[
    \nabla_\theta \log(1-\sigma(\thetab^\T\xx)) = -\xx\sigma(\thetab^\T\xx).
\]

Prove to yourself why this is correct. Thus the derivative of our objective $\Lc(\thetab)$
becomes
\[
    \nabla_\theta \Lc = \sum_{i=1}^m \left[  \yy_i(1-\sigma(\thetab^\T\xx)) - (1-\yy_i)\sigma(\thetab^\T\xx)  \right]\xx_i = \sum_{i=1}^m \left[ \yy_i - \sigma(\thetab^\T\xx) \right]\xx.
\]
This can be input into any standard gradient-based optimization algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithms for Solving Convex Problems}

\subsection{Gradient Descent}

\subsection{Newton's Method}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Duality}

\subsection{Duality Gap}

\subsection{The Lagrange Dual Function}

\subsection{Finding the Dual}

\subsubsection{The Dual of The Linear Program}

\subsection{Solving Problems Using Duality}

\subsubsection{Minimize a Quadratic Under Quadratic Constraints}

\subsection{Complementary Slackness of Duality}

\subsubsection{Solving The Dual Linear Program via a Primal Solution}

\subsection{The KKT (Karush-Kuhn-Tucker) Conditions}


\end{document}
