\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{../../materials/section/macros.tex}

% info for header block in upper right hand corner
\name{------}
\class{Math 189r}
\assignment{Homework 1}
\duedate{September 19, 2016}

\begin{document}

There are 8 problems in this set. You must complete 6 (doing more will get no
credit - work on your project!) 3 of the problems (you choose) are due on September 12,
and the rest of the problems you complete are due on September 19. Feel
free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
When implementing algorithms you may not use any library (such as \texttt{sklearn})
that already implements the algorithms but you may use any other library for
data cleaning and numeric purposes (\texttt{numpy} or \texttt{pandas}). Use common
sense.
Problems are in no specific order.\\[1em]


\textbf{1} (\textbf{regression}). Download the data at 
\url{https://math189r.github.io/hw/data/online_news_popularity/online_news_popularity.csv}
and the info file at
\url{https://math189r.github.io/hw/data/online_news_popularity/online_news_popularity.txt}.
Read the info file. Split the csv file into a training and test set with
the first two thirds of the data in the training set and the rest for testing.
Of the testing data, split the first half into a `validation set' (used
to optimize hyperparameters while leaving your testing data pristine) and
the remaining half as your test set.
We will use this data for the remainder of the problem. The goal of this data
is to predict the \textbf{log} number of shares a news article will have given the other
features.
\begin{enumerate}[(a)]
    \item (\textbf{math}) Find a closed form solution $\xx^\star$ to the ridge regression
        problem:
        \[
            \text{minimize: } ||A\xx - \bb||_2^2 + ||\Gamma\xx||_2^2.
        \]
    \item (\textbf{implementation}) Attempt to predict the $\log\text{shares}$ using ridge
        regression from the previous problem solution. Make sure you include a bias
        term and \textit{don't regularize the bias term}.
        Find the optimal regularization parameter $\lambda$
        from the validation set. Plot both $\lambda$ versus the validation RMSE (you should have
        tried at least 150 parameter settings randomly chosen between 0.0 and 150.0 because
        the dataset is small)
        and $\lambda$ versus $||\thetab^\star||_2$ where $\thetab$ is your weight vector.
        What is the final RMSE on the test set with the optimal $\lambda^\star$?
    \item (\textbf{math}) Consider regularized linear regression where we pull the
        basis term out of the feature vectors. That is, instead of computing $\hat\yy
        = \thetab^\T\xx$ with $\xx_0 = 1$, we compute $\hat\yy = \thetab^\T\xx + b$.
        This corresponds to solving the optimization problem
        \[
            \text{minimize: } ||A\xx + b\1 - \yy||_2^2 + ||\Gamma\xx||_2^2.
        \]
        Solve for the optimal $\xx^\star$ explicitly. Use this close form to compute the
        bias term for the previous problem (with the same regularization strategy). Make
        sure it is the same.
    \item (\textbf{implementation}) We can also compute the solution to the least squares
        problem using gradient descent. Consider the same bias-relocated objective
        \[
            \text{minimize: } f = ||A\xx + b\1 - \yy||_2^2 + ||\Gamma\xx||_2^2.
        \]
        Compute the gradients and 
\end{enumerate}

\textbf{2} (\textbf{iterative optimization}) This problem will use data from
\url{https://math189r.github.io/hw/data/bike-sharing/hour.csv}
\clearpage


\end{document}
